{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a211ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "from category_encoders import TargetEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbb4fcd",
   "metadata": {},
   "source": [
    "# 1.0 Introduction\n",
    "\n",
    "### Project Context\n",
    "This notebook initiates a series dedicated to the Machine Learning component of the project. An exploratory data analysis (EDA) has been previously conducted and is available on GitHub alongside this code. This EDA provides foundational insights for the project, outlining essential preprocessing steps such as identifying columns for removal, addressing anomalies in the Price column, and initial assessments of feature characteristics.  \n",
    "<br>\n",
    "This dataset contains information about Airbnb properties in the London region as of September 2022. As a snapshot in time, it does not provide information on price variations over months. The dataset includes details such as nightly prices, locations, host information, reviews, property types (private or shared), amenities, and more.  \n",
    "<br>\n",
    "Dataset was acquired from Kaggle:  \n",
    "https://www.kaggle.com/datasets/mrnabiz/detailed-airbnb-listing-data-london-sep-2022  \n",
    "This public dataset is part of Airbnb and can be accessed at the original source link: http://insideairbnb.com/  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82a3f8",
   "metadata": {},
   "source": [
    "<span style=\"font-size: smaller;\">**Note**:\n",
    "This project is primarily intended for learning purposes. Initially, the EDA phase encompassed the entire dataset because there was no plan for a subsequent machine learning component. As per best practices, EDA should ideally be conducted exclusively on the training dataset to ensure model validity and reliability for production deployment. Conducting EDA on the entire dataset may lead to evaluation metrics that overestimate the model's generalization to new, unseen data. Despite this initial approach, the project continues with awareness of these implications, prioritizing learning and understanding.\n",
    "</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a32aad",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648a6a29",
   "metadata": {},
   "source": [
    "### Development Plan\n",
    "As we continue focusing on the Machine Learning component of the project, we are establishing the framework for model evaluation. The target variable will be the Price of each property, and Mean Absolute Error has been selected as the primary evaluation metric for the models. This choice followed the identification of outliers and positively skewed distributions during the EDA.  \n",
    "<br>\n",
    "The primary objective of this initial notebook is to **establish a simple baseline model**. This model will serve as a reference for evaluating more advanced models that are expected to achieve lower Mean Absolute Error values. To create this baseline, we will employ a Dummy Regressor.  \n",
    "<br>\n",
    "Additionally, we will **develop a comprehensive data processing pipeline that includes feature engineering and preprocessing stages**. While these steps do not impact the performance of a Dummy Regressor, they are crucial for evaluating more advanced machine learning models in later stages of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d1b62",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60158359",
   "metadata": {},
   "source": [
    "### Data import and initial processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7323e65",
   "metadata": {},
   "source": [
    "Based on the EDA, some columns are deemed unnecessary and will therefore be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92566be2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../21 LondonAirbnb - EDA/clean_df.csv',\n",
    "                 parse_dates=[\n",
    "                     'first_review',\n",
    "                     'last_review',\n",
    "                     'host_since',\n",
    "                 ])\n",
    "df.drop(columns=[\n",
    "    'id',\n",
    "    'listing_url',\n",
    "    'neighborhood_overview',\n",
    "    'host_id',\n",
    "    'host_url',\n",
    "    'host_name',\n",
    "    'host_picture_url',\n",
    "    'property_type',\n",
    "    'calculated_host_listings_count',\n",
    "    'calculated_host_listings_count_entire_homes',\n",
    "    'calculated_host_listings_count_private_rooms',\n",
    "    'calculated_host_listings_count_shared_rooms',\n",
    "    'minimum_nights_avg_ntm',\n",
    "    'maximum_nights_avg_ntm',\n",
    "    'host_location',\n",
    "    'host_about',\n",
    "    ], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b17966",
   "metadata": {},
   "source": [
    "Based on the EDA, we found that null and zero ('$0.00') Price values are rare and will be deleted.  \n",
    "All Price values will also be converted to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6648f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'] = df['price'].replace('$0.00', np.nan)\n",
    "df = df.dropna(subset=['price'])\n",
    "df['price'] = (df['price'].str.replace('$', '', regex='False')\n",
    "               .str.replace(',', '', regex='False')\n",
    "               .astype(float, copy=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010df8f4",
   "metadata": {},
   "source": [
    "### Partition the data into Train and Test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e9b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=777)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63794c87",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48cf3fd",
   "metadata": {},
   "source": [
    "# 2.0 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb9866",
   "metadata": {},
   "source": [
    "This project will use a straightforward and **simple feature engineering process that can precede preprocessing** seamlessly.  \n",
    "<br>\n",
    "This approach involves creating new features, applying basic encoding to the 'host_response_time' column, and converting numeric data currently stored as text into suitable formats. Each newly generated feature will replace its original column. The feature engineering process includes:\n",
    "  \n",
    "* Incorporating all features identified during the EDA phase.\n",
    "* Generating new features for each datetime column to indicate the elapsed time from a reference date.\n",
    "* Replacing the 'host_verification' column with separate columns for each of the most common host verification methods.\n",
    "* Introducing a new 'distance_to_palace' column by merging the 'latitude' and 'longitude' columns for enhanced location-based analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b9fa0",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a120c",
   "metadata": {},
   "source": [
    "Declare functions that will be employed to generate new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a60a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_shared_bathroom(row):\n",
    "    \"\"\"Returns 't' if word 'shared' is in column 'bathrooms_text'.  \n",
    "    \n",
    "    Returns 'f' otherwise.\n",
    "    \"\"\"\n",
    "    if pd.isnull(row['bathrooms_text']):\n",
    "        return np.nan\n",
    "    elif 'shared' in row['bathrooms_text'].lower():\n",
    "        return 't'\n",
    "    else:\n",
    "        return 'f'\n",
    "\n",
    "\n",
    "def calc_number_of_bathrooms(row):\n",
    "    \"\"\"Returns the number of bathrooms in column 'bathrooms_text'.  \n",
    "    \n",
    "    Obs: incomplete bathrooms are counted as 0.5 bathroom.\n",
    "    \"\"\"\n",
    "    if pd.isnull(row['bathrooms_text']):\n",
    "        return np.nan\n",
    "    elif 'half-bath' in row['bathrooms_text'].lower():\n",
    "        return 0.5\n",
    "    else:\n",
    "        return float(row['bathrooms_text'].split()[0])\n",
    "\n",
    "\n",
    "def calc_elapsed_time(row, column, time_unit='months'):\n",
    "    \"\"\"Returns the elapsed time since a date stored in the specified row and column.  \n",
    "    \n",
    "    'time_unit' can be either 'months'(default) or 'years'.\n",
    "    \"\"\"\n",
    "    valid_time_units = {'months', 'years'}\n",
    "    if time_unit not in valid_time_units:\n",
    "        raise ValueError('order_by: must be one of %r.' % valid_orders)\n",
    "    elif pd.isnull(row[column]):\n",
    "        return np.nan\n",
    "    elif time_unit == 'months':\n",
    "        return (12*(2022-row[column].year) + (9-row[column].month))\n",
    "    else:\n",
    "        return (2022 - row[column].year)\n",
    "\n",
    "\n",
    "def calc_number_of_amenities(row):\n",
    "    \"\"\"Returns the number of amenities stored in column 'amenities'.\"\"\"\n",
    "    if pd.isnull(row['amenities']):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return len(row['amenities'].strip('[]\"').split('\", \"'))\n",
    "\n",
    "\n",
    "def has_host_verification(row, verification_type):\n",
    "    \"\"\"Returns 't' if inputed verification_type is in column 'host_verifications'.  \n",
    "    \n",
    "    Returns 'f' otherwise.\n",
    "    \"\"\"\n",
    "    if pd.isnull(row['host_verifications']):\n",
    "        return np.nan\n",
    "    elif verification_type in row['host_verifications']:\n",
    "        return 't'\n",
    "    else:\n",
    "        return 'f'\n",
    "\n",
    "\n",
    "def distance_from_palace(row):\n",
    "    \"\"\"Calculates distance in km from Buckingham Palace.  \n",
    "    \n",
    "    Columns 'longitude' and 'latitude' define the point that the distance is\n",
    "    calculated from.\"\"\"\n",
    "    def haversine(lon1, lat1, lon2, lat2):\n",
    "        \"\"\"\n",
    "        Calculate the great circle distance between two points\n",
    "        on the earth (specified in decimal degrees).\n",
    "        \"\"\"\n",
    "        # Convert decimal degrees to radians\n",
    "        lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "        # Haversine formula\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "        km = 6371 * c\n",
    "        return km\n",
    "    \n",
    "    \n",
    "    if pd.isnull(row['longitude']):\n",
    "        return np.nan\n",
    "    elif pd.isnull(row['latitude']):\n",
    "        return np.nan\n",
    "    else:\n",
    "        palace_lon, palace_lat = -0.140634, 51.501476\n",
    "        lon = row['longitude']\n",
    "        lat = row['latitude']\n",
    "        return haversine(palace_lon, palace_lat, lon, lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03bf517",
   "metadata": {},
   "source": [
    "Map to encode the 'host_response_time' column proportionally based on the number of hours of response time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "761a57f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_time_map = {\n",
    "    'within an hour': 1,\n",
    "    'within a few hours': 6,\n",
    "    'within a day': 24,\n",
    "    'a few days or more': 72,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08193c5",
   "metadata": {},
   "source": [
    "Declare function to convert numeric data currently stored as text into float format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa2eaf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_percentage(row, column):\n",
    "        if pd.isnull(row[column]):\n",
    "            return np.nan\n",
    "        try:\n",
    "            return float(row[column].strip('%')) / 100.0\n",
    "        except (ValueError, AttributeError):\n",
    "            return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89fc60f",
   "metadata": {},
   "source": [
    "### Creating FeatureEngineering transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf69f6d",
   "metadata": {},
   "source": [
    "We are developing a custom transformer that subclasses BaseEstimator and TransformerMixin from scikit-learn, ensuring seamless integration into scikit-learn pipelines. This approach simplifies cross-validation and hyperparameter tuning, thereby enhancing model performance consistency across various configurations and preventing data leakage.  \n",
    "<br>\n",
    "The current version of the custom transformer supports the **encode_hrt** setting, which can be toggled between True or False. The code structure allows for easy integration of additional settings to explore various feature engineering possibilities.  \n",
    "<br>\n",
    "<span style=\"font-size: smaller;\"> Note: by default, the value of **encode_hrt** is set to True. When set to False, the 'host_response_time' column remains categorical and is not encoded during feature engineering. Consequently, it is treated as a categorical column during preprocessing.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48410c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer for feature engineering, compatible with scikit-learn pipelines.  \n",
    "    \n",
    "    This transformer subclasses BaseEstimator and TransformerMixin from scikit-learn,\n",
    "    ensuring compatibility with scikit-learn pipelines. It facilitates streamlined\n",
    "    integration into machine learning workflows, supporting cross-validation,\n",
    "    hyperparameter tuning, and preventing data leakage.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    encode_hrt : bool, default=True\n",
    "        Whether to encode the 'host_response_time' column during feature engineering.\n",
    "        If True, 'host_response_time' is encoded; if False, it remains categorical.\n",
    "   \n",
    "    \"\"\"\n",
    "    def __init__(self, encode_hrt=True):\n",
    "        self.encode_hrt = encode_hrt\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_names_ = X.columns.tolist()\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Create new features:\n",
    "        X['shared_bathroom'] = X.apply(calc_shared_bathroom, axis=1)\n",
    "        X['number_of_bathrooms'] = X.apply(calc_number_of_bathrooms, axis=1)\n",
    "        X['number_of_amenities'] = X.apply(calc_number_of_amenities, axis=1)\n",
    "        X['host_experience_y'] = X.apply(\n",
    "            lambda row: calc_elapsed_time(row, 'host_since', 'years'), axis=1)\n",
    "        X['months_first_rev'] = X.apply(\n",
    "            lambda row: calc_elapsed_time(row, 'first_review'), axis=1)\n",
    "        X['months_last_rev'] = X.apply(\n",
    "            lambda row: calc_elapsed_time(row, 'last_review'), axis=1)\n",
    "        lst_verif_types = [\n",
    "            'phone',\n",
    "            'email',\n",
    "            'work_email',\n",
    "        ]\n",
    "        for verification_type in lst_verif_types:\n",
    "            X['host_verif_'+verification_type] = X.apply(\n",
    "                lambda row:  has_host_verification(row, verification_type), axis=1)\n",
    "        X['distance_to_palace'] = X.apply(distance_from_palace, axis=1)\n",
    "        \n",
    "        # Transform categorical features into numerical features:\n",
    "        if self.encode_hrt == True:\n",
    "            X['host_response_time'] = (X['host_response_time']\n",
    "                                            .map(response_time_map))\n",
    "        \n",
    "        # Transform numbers saved as strings into numbers:\n",
    "        X['host_response_rate'] = (X.apply(\n",
    "            lambda row: convert_percentage(row, 'host_response_rate'), axis=1))\n",
    "        X['host_acceptance_rate'] = (X.apply(\n",
    "            lambda row: convert_percentage(row, 'host_acceptance_rate'), axis=1))\n",
    "    \n",
    "        # Remove the columns that were originally used to generate new\n",
    "        # features, as they are no longer necessary:\n",
    "        X.drop(columns = [\n",
    "            'name',\n",
    "            'description',\n",
    "            'bathrooms_text',\n",
    "            'amenities',\n",
    "            'host_since',\n",
    "            'first_review',\n",
    "            'last_review',\n",
    "            'host_verifications',\n",
    "            'latitude',\n",
    "            'longitude',\n",
    "            ], axis = 1, inplace = True)\n",
    "        \n",
    "        self.feature_names_ = X.columns.tolist()\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "            if input_features is None:\n",
    "                input_features = self.feature_names_\n",
    "            return input_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b0bc9c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ee540",
   "metadata": {},
   "source": [
    "# 3.0 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeae980a",
   "metadata": {},
   "source": [
    "To demonstrate the functionality of automatic column type detection within the pipeline, we will apply a minimal preprocessing step. This step categorizes the data into true/false, categorical, and numeric columns, each undergoing specific processing: \n",
    "<br>\n",
    "- True/False values will be converted to binary (ones and zeros).\n",
    "- Category data will be encoded using TargetEncoder.\n",
    "- Numeric data will undergo no preprocessing at the moment.  \n",
    "\n",
    "In subsequent stages, imputers and scalers will be integrated into the pipeline to explore more complex models. It's important to note that different data types and models may necessitate varying strategies for imputation and scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a8dd9",
   "metadata": {},
   "source": [
    "#### True/False columns preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2282b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFToBinaryTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer to convert 't' and 'f' values to binary (1 and 0).\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        return X.replace({'t': 1, 'f': 0})\n",
    "    \n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b84593c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_transformer = Pipeline(steps=[\n",
    "    ('tf_to_binary', TFToBinaryTransformer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33caf5aa",
   "metadata": {},
   "source": [
    "#### Category columns preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e99b50b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_transformer = Pipeline(steps=[\n",
    "    ('encoder', TargetEncoder()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a0ce81",
   "metadata": {},
   "source": [
    "#### Numeric columns preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e87c1eae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_transformer = Pipeline(steps=[\n",
    "    ('pass', 'passthrough'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d196fc62",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0334f",
   "metadata": {},
   "source": [
    "### UpdateColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a36d47",
   "metadata": {},
   "source": [
    "This transformer facilitates automatic column type detection within the pipeline. It dynamically identifies the correct lists of columns for preprocessing based on their types at the time preprocessing occurs.  \n",
    "<br>\n",
    "Due to the nature of Feature Engineering, which generates new columns within the pipeline, column type detection cannot be performed before the pipeline begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccd108a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdateColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer that updates the column grouping in a preprocessor\n",
    "    based on data characteristics.  \n",
    "\n",
    "    This transformer subclasses BaseEstimator and TransformerMixin from\n",
    "    scikit-learn, making it compatible with scikit-learn pipelines. It\n",
    "    dynamically categorizes columns into 'tf' (true/false), 'cat' (categorical),\n",
    "    and 'num' (numerical) groups based on their data type and values, and\n",
    "    updates the preprocessing steps accordingly.\n",
    "    \"\"\"\n",
    "    def __init__(self, preprocessor):\n",
    "        self.preprocessor = preprocessor\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # True/False columns contain only 't' and 'f' values or may have missing values.\n",
    "        tf_features = [col for col in (X.select_dtypes(include=['object', 'category'])\n",
    "                                       .columns)\n",
    "                       if set(X[col].dropna().unique()) <= {'t', 'f'}]\n",
    "        # All remaining columns of object or category type will be processed together.\n",
    "        cat_features = [col for col in (X.select_dtypes(include=['object', 'category'])\n",
    "                                       .columns)\n",
    "                       if col not in tf_features]\n",
    "        # Numeric type columns will be processed together\n",
    "        numeric_features = X.select_dtypes(['number']).columns.to_list()\n",
    "        \n",
    "        new_transformers = []\n",
    "        for name, transformer, columns in self.preprocessor.transformers:\n",
    "            if name == 'cat':\n",
    "                new_transformers.append((name, transformer, cat_features))\n",
    "            elif name == 'tf':\n",
    "                new_transformers.append((name, transformer, tf_features))\n",
    "            elif name == 'num':\n",
    "                new_transformers.append((name, transformer, numeric_features))\n",
    "            else:\n",
    "                new_transformers.append((name, transformer, columns))\n",
    "        self.preprocessor.transformers = new_transformers\n",
    "        \n",
    "        return self.preprocessor.fit(X, y)\n",
    "    \n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.preprocessor.transform(X)\n",
    "    \n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.preprocessor.get_feature_names_out(input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd895f7",
   "metadata": {},
   "source": [
    "### Preprocessor\n",
    "Preprocessor is initialized without listing any columns, as the list of columns will be updated by UpdateColumnTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4cfadf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', cat_transformer, []),\n",
    "        ('tf', tf_transformer, []),\n",
    "        ('num', num_transformer, []),\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff14ae3c",
   "metadata": {},
   "source": [
    "# 4.0 Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f7060b",
   "metadata": {},
   "source": [
    "Pipeline integrates feature engineering and preprocessing components, using a Dummy Regressor as the baseline estimator. This Dummy Regressor consistently predicts the target value as the median of the training dataset used to fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37720b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('feature_engineering', FeatureEngineering()),\n",
    "    ('type_detection', UpdateColumnTransformer(preprocessor)),\n",
    "    ('regressor', DummyRegressor(strategy='median')),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b277d",
   "metadata": {},
   "source": [
    "# 5.0 Baseline Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739271c",
   "metadata": {},
   "source": [
    "Given the identification of outliers and positively skewed distributions during the EDA in both the features and target values, the chosen primary evaluation metric for the models is Mean Absolute Error (MAE). MAE is preferred for its robustness to outliers and its straightforward measure of average prediction error.  \n",
    "<br>\n",
    "Additionally, Root Mean Squared Error (RMSE) will be considered as an auxiliary evaluation metric. RMSE provides a measure of the spread of prediction errors, offering a more sensitive evaluation of prediction accuracy. This dual metric approach enhances our understanding of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3497f73b",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "We will use cross-validation to measure the baseline values of the selected metric, Mean Absolute Error (MAE), and the auxiliary evaluation metric, Root Mean Square Error (RMSE).\n",
    "<br>\n",
    "This approach provides an estimate of the evaluation metric without relying on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd34d235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Average MAE: 119.67833952398125\n",
      "Baseline Average RMSE: 411.2180994583384\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=777)\n",
    "\n",
    "avg_mae = 0\n",
    "avg_rmse = 0\n",
    "for tr, ts in kf.split(X_train, y_train):\n",
    "    Xtr, Xvl = X_train.iloc[tr], X_train.iloc[ts]\n",
    "    ytr, yvl = y_train.iloc[tr], y_train.iloc[ts]\n",
    "    \n",
    "    pipeline.fit(Xtr, ytr)\n",
    "    ypred = pipeline.predict(Xvl)\n",
    "    avg_mae += mean_absolute_error(yvl, ypred)\n",
    "    avg_rmse += root_mean_squared_error(yvl, ypred)\n",
    "\n",
    "print('Baseline Average MAE:', avg_mae/5)\n",
    "print('Baseline Average RMSE:', avg_rmse/5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
